{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import json\n",
    "import csv\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def det_type(da):\n",
    "    #goal: determine the type and create a list of index for the kanji, radicals, and vocab in a lesson\n",
    "    #input: da - data pulled from wanikani\n",
    "    #output: k - list of kanji indexes in lesson\n",
    "    #        r - list of radical indexes in lesson\n",
    "    #        v - list of vocab indexes in lesson\n",
    "    \n",
    "    count = 0       #initialize index reference\n",
    "    k=[]            #initialize list for each type\n",
    "    r=[]\n",
    "    v=[]\n",
    "    for i in da:            #go through each item in the data; determine the type; add the index to the appropriate list\n",
    "        data = i['data']\n",
    "        #print(i['id'])\n",
    "        if i['object'] == 'kanji':\n",
    "            k.append(count)\n",
    "        elif i['object'] == 'vocabulary':\n",
    "            v.append(count)\n",
    "        elif i['object'] == 'radical':\n",
    "            r.append(count)\n",
    "        else:                               #added as a precaution in case there are any items without a type\n",
    "            print('extra count' +count)\n",
    "        count = count+1\n",
    "    return k, r, v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_kanjidf(k, da):\n",
    "    #goal: create the kanji dataframe with all of the kanji for one lesson and clean it up so it can easily be turned into a deck in anki\n",
    "    #input: k - list of indexes for the kanji in the lesson\n",
    "    #       da - the data pulled from wanikani\n",
    "    #output: kanjid - the cleaned up dataframe with all the necessary information needed to create a useful anki deck\n",
    "    \n",
    "    kanjid = pd.DataFrame(columns = ['id','object','updated','url','level','characters','meanings','aux_mean',\n",
    "                                 'readings','lesson_pos','mean_mne','read_mne','mean_hint','read_hint'])       #create the kanji dataframe with column names\n",
    "    \n",
    "    #minimum = 100\n",
    "    #maximum = -100\n",
    "\n",
    "    for kanji in k:  #go through each kanji to pull out the important data\n",
    "        \n",
    "        #print(\"kanji meaning length\" + str(len(da[kanji]['data']['meanings'])))         #Determine the max and min number of meanings for kanji in current lesson\n",
    "        #if len(da[kanji]['data']['meanings']) < minimum:\n",
    "        #    minimum = len(da[kanji]['data']['meanings'])\n",
    "        #if len(da[kanji]['data']['meanings']) > maximum:\n",
    "        #    maximum = len(da[kanji]['data']['meanings'])\n",
    "        \n",
    "        meaning = []\n",
    "        readings = []\n",
    "        aux_mean = []\n",
    "        for item in da[kanji]['data']['meanings']:\n",
    "            meaning.append(item['meaning'])\n",
    "        for itm in da[kanji]['data']['readings']:\n",
    "            if itm['accepted_answer']:\n",
    "                readings.append(itm['reading'])\n",
    "        if da[kanji]['data']['auxiliary_meanings']:\n",
    "            for i in da[kanji]['data']['auxiliary_meanings']:\n",
    "                if i['type'] == 'whitelist':\n",
    "                    aux_mean.append(i['meaning'])\n",
    "\n",
    "        \n",
    "        kanjid = kanjid.append({'id': da[kanji]['id'],                          #append the important data to the dataframe created above\n",
    "                                  'object':da[kanji]['object'],\n",
    "                                  'updated': da[kanji]['data_updated_at'],\n",
    "                                  'level': da[kanji]['data']['level'],\n",
    "                                  'url': da[kanji]['data']['document_url'], \n",
    "                                  'characters': da[kanji]['data']['characters'],\n",
    "                                  'meanings':meaning,\n",
    "                                  'aux_mean': aux_mean,\n",
    "                                  'readings': readings,\n",
    "                                  'lesson_pos': da[kanji]['data']['lesson_position'],\n",
    "                                  'mean_mne': da[kanji]['data']['meaning_mnemonic'],\n",
    "                                  'read_mne': da[kanji]['data']['reading_mnemonic'],\n",
    "                                  'mean_hint': da[kanji]['data']['meaning_hint'],\n",
    "                                  'read_hint': da[kanji]['data']['reading_hint']\n",
    "                                 },\n",
    "                       ignore_index=True)\n",
    "\n",
    "    #print(\"Kanji \\nmax = \" + str(maximum) + \" minimum = \" + str(minimum)) \n",
    "    return kanjid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vocabdf(v, da):\n",
    "    #goal: create the vocab dataframe with all of the vocab for one lesson and clean it up so it can easily be turned into a deck in anki\n",
    "    #input: v - list of indexes for the vocab in the lesson\n",
    "    #       da - the data pulled from wanikani\n",
    "    #output: vocabd - the cleaned up dataframe with all the necessary information needed to create a useful anki deck\n",
    "    \n",
    "    vocabd = pd.DataFrame(columns = ['id','object','updated','url','level','characters','meanings','aux_mean','readings',\n",
    "                                 'lesson_pos','part_sp','mean_mne','read_mne','context_sent_ja1', 'context_sent1_en1', \n",
    "                                     'context_sent1_ja2', 'context_sent1_en2', 'context_sent1_ja3', 'context_sent1_en3','audio'])             #create the vobab dataframe with column names\n",
    "    maximum = -100\n",
    "    minimum = 100\n",
    "    for vocab in v:         #go through each vocab to pull out the important data\n",
    "        \n",
    "        if len(da[vocab]['data']['auxiliary_meanings']) < minimum:       #Determine the max and min number of meanings for vocab in current lesson\n",
    "            minimum = len(da[vocab]['data']['auxiliary_meanings'])\n",
    "        if len(da[vocab]['data']['auxiliary_meanings']) > maximum:\n",
    "            maximum = len(da[vocab]['data']['auxiliary_meanings'])\n",
    "        \n",
    "        meaning = []\n",
    "        readings = []\n",
    "        aux_mean = []\n",
    "        for item in da[vocab]['data']['meanings']:\n",
    "            meaning.append(item['meaning'])\n",
    "        for itm in da[vocab]['data']['readings']:\n",
    "            if itm['accepted_answer']:\n",
    "                readings.append(itm['reading'])\n",
    "        if da[vocab]['data']['auxiliary_meanings']:\n",
    "            for i in da[vocab]['data']['auxiliary_meanings']:\n",
    "                if i['type'] == 'whitelist':\n",
    "                    aux_mean.append(i['meaning'])\n",
    "                \n",
    "        if len(da[vocab]['data']['context_sentences']) == 1:\n",
    "                vocabd = vocabd.append({'id': da[vocab]['id'],              #append the important data to the dataframe created above\n",
    "                              'object':da[vocab]['object'],\n",
    "                              'updated': da[vocab]['data_updated_at'],\n",
    "                              'level': da[vocab]['data']['level'],\n",
    "                              'url': da[vocab]['data']['document_url'], \n",
    "                              'characters': da[vocab]['data']['characters'],\n",
    "                              'meanings': meaning,\n",
    "                              'aux_mean': aux_mean,\n",
    "                              'readings': readings,\n",
    "                              'lesson_pos': da[vocab]['data']['lesson_position'],\n",
    "                              'part_sp': da[vocab]['data']['parts_of_speech'],\n",
    "                              'mean_mne': da[vocab]['data']['meaning_mnemonic'],\n",
    "                              'read_mne': da[vocab]['data']['reading_mnemonic'],\n",
    "                              'context_sent_ja1': da[vocab]['data']['context_sentences'][0]['ja'],\n",
    "                              'context_sent_en1': da[vocab]['data']['context_sentences'][0]['en'],\n",
    "                              'audio': da[vocab]['data']['pronunciation_audios']\n",
    "                              },\n",
    "                   ignore_index=True)\n",
    "        elif len(da[vocab]['data']['context_sentences']) == 2:\n",
    "            \n",
    "            vocabd = vocabd.append({'id': da[vocab]['id'],              #append the important data to the dataframe created above\n",
    "                                  'object':da[vocab]['object'],\n",
    "                                  'updated': da[vocab]['data_updated_at'],\n",
    "                                  'level': da[vocab]['data']['level'],\n",
    "                                  'url': da[vocab]['data']['document_url'], \n",
    "                                  'characters': da[vocab]['data']['characters'],\n",
    "                                  'meanings': meaning,\n",
    "                                  'aux_mean': aux_mean,\n",
    "                                  'readings': readings,\n",
    "                                  'lesson_pos': da[vocab]['data']['lesson_position'],\n",
    "                                  'part_sp': da[vocab]['data']['parts_of_speech'],\n",
    "                                  'mean_mne': da[vocab]['data']['meaning_mnemonic'],\n",
    "                                  'read_mne': da[vocab]['data']['reading_mnemonic'],\n",
    "                                  'context_sent_ja1': da[vocab]['data']['context_sentences'][0]['ja'],\n",
    "                                  'context_sent_en1': da[vocab]['data']['context_sentences'][0]['en'],\n",
    "                                  'context_sent_ja2': da[vocab]['data']['context_sentences'][1]['ja'],\n",
    "                                  'context_sent_en2': da[vocab]['data']['context_sentences'][1]['en'],\n",
    "                                  'audio': da[vocab]['data']['pronunciation_audios']\n",
    "                                  },\n",
    "                       ignore_index=True)\n",
    "        else:\n",
    "            vocabd = vocabd.append({'id': da[vocab]['id'],              #append the important data to the dataframe created above\n",
    "                                  'object':da[vocab]['object'],\n",
    "                                  'updated': da[vocab]['data_updated_at'],\n",
    "                                  'level': da[vocab]['data']['level'],\n",
    "                                  'url': da[vocab]['data']['document_url'], \n",
    "                                  'characters': da[vocab]['data']['characters'],\n",
    "                                  'meanings': meaning,\n",
    "                                  'aux_mean': aux_mean,\n",
    "                                  'readings': readings,\n",
    "                                  'lesson_pos': da[vocab]['data']['lesson_position'],\n",
    "                                  'part_sp': da[vocab]['data']['parts_of_speech'],\n",
    "                                  'mean_mne': da[vocab]['data']['meaning_mnemonic'],\n",
    "                                  'read_mne': da[vocab]['data']['reading_mnemonic'],\n",
    "                                  'context_sent_ja1': da[vocab]['data']['context_sentences'][0]['ja'],\n",
    "                                  'context_sent_en1': da[vocab]['data']['context_sentences'][0]['en'],\n",
    "                                  'context_sent_ja2': da[vocab]['data']['context_sentences'][1]['ja'],\n",
    "                                  'context_sent_en2': da[vocab]['data']['context_sentences'][1]['en'],\n",
    "                                  'context_sent_ja3': da[vocab]['data']['context_sentences'][2]['ja'],\n",
    "                                  'context_sent_en3': da[vocab]['data']['context_sentences'][2]['ja'],\n",
    "                                  'audio': da[vocab]['data']['pronunciation_audios']\n",
    "                                  },\n",
    "                       ignore_index=True)\n",
    "\n",
    "    #print(\"Vocab \\nmax = \" + str(maximum) + \" minimum = \" + str(minimum))\n",
    "    return vocabd#, maximum, minimum\n",
    "    #print(vocabdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_radicaldf(r, da):\n",
    "    #goal: create the radical dataframe with all of the radicals for one lesson and clean it up so it can easily be turned into a deck in anki\n",
    "    #input: r - list of indexes for the radicals in the lesson\n",
    "    #       da - the data pulled from wanikani\n",
    "    #output: radicald - the cleaned up dataframe with all the necessary information needed to create a useful anki deck\n",
    "    \n",
    "    radicald = pd.DataFrame(columns = ['id','object','updated','url','level','characters','meanings','lesson_pos',\n",
    "                                   'mean_mne'])                                                                #create the radical dataframe with column names\n",
    "    #maximum = -100\n",
    "    #minimum = 100\n",
    "    \n",
    "    for radical in r:           #go through each radical to pull out the important data\n",
    "        \n",
    "        #if len(da[radical]['data']['meanings']) < minimum:             #Determine the max and min number of meanings for radicals in current lesson\n",
    "        #    minimum = len(da[radical]['data']['meanings'])\n",
    "        #if len(da[radical]['data']['meanings']) > maximum:\n",
    "        #    maximum = len(da[radical]['data']['meanings'])\n",
    "        \n",
    "        if da[radical]['data']['characters'] != None:               #append the important data to the dataframe created above\n",
    "            radicald = radicald.append({'id': da[radical]['id'],\n",
    "                                      'object':da[radical]['object'],\n",
    "                                      'updated': da[radical]['data_updated_at'],\n",
    "                                      'level': da[radical]['data']['level'],\n",
    "                                      'url': da[radical]['data']['document_url'], \n",
    "                                      'characters': da[radical]['data']['characters'],\n",
    "                                      'meanings':da[radical]['data']['meanings'][0]['meaning'],\n",
    "                                      'lesson_pos': da[radical]['data']['lesson_position'],\n",
    "                                      #'char_img': da[radical]['data']['character_images'][0]['url'],\n",
    "                                      'mean_mne': da[radical]['data']['meaning_mnemonic']\n",
    "                                     },\n",
    "                       ignore_index=True)\n",
    "        else:\n",
    "            radicald = radicald.append({'id': da[radical]['id'],\n",
    "                                      'object':da[radical]['object'],\n",
    "                                      'updated': da[radical]['data_updated_at'],\n",
    "                                      'level': da[radical]['data']['level'],\n",
    "                                      'url': da[radical]['data']['document_url'][0], \n",
    "                                      'characters': da[radical]['data']['characters'],\n",
    "                                      'meanings':da[radical]['data']['meanings'][0]['meaning'],\n",
    "                                      'lesson_pos': da[radical]['data']['lesson_position'],\n",
    "                                      'char_img': da[radical]['data']['character_images'][0]['url'],\n",
    "                                      'mean_mne': da[radical]['data']['meaning_mnemonic']\n",
    "                                     },\n",
    "                       ignore_index=True)\n",
    "        \n",
    "    #print(\"Radical \\nmax = \" + str(maximum) + \" minimum = \" + str(minimum))\n",
    "    #print(radicaldf)\n",
    "    return radicald"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(level):\n",
    "    #goal: pull the data for the selected level from the wanikani api\n",
    "    #input: level - the desired level to pull the data from\n",
    "    #output: dja - the data from the reuest returned in an easy way to extract the desired information\n",
    "    \n",
    "    response = requests.get(\"https://api.wanikani.com/v2/subjects?levels=\"+ str(level), headers = {\"Authorization\": \"Bearer c61d70a7-c134-4b4a-bb05-f97a7c59af8b\"})\n",
    "    dj = response.json()    #retrieve the json from the website\n",
    "    dja = dj['data']     #use only the data returned \n",
    "    \n",
    "    return dja\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\lib\\site-packages\\pandas\\core\\frame.py:6211: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n"
     ]
    }
   ],
   "source": [
    "levelst = 1             #level to start pulling data from\n",
    "levele = 60             #level to stop pulling data from\n",
    "kanjidf = pd.DataFrame(columns = ['id','object','updated','url','level','characters','meanings','aux_mean',\n",
    "                                 'readings','lesson_pos','mean_mne','read_mne','mean_hint','read_hint'])       #create the kanji dataframe with column names\n",
    "vocabdf = pd.DataFrame(columns = ['id','object','updated','url','level','characters','meanings','aux_mean','readings',\n",
    "                                 'lesson_pos','part_sp','mean_mne','read_mne','context_sent_ja1', 'context_sent_en1', \n",
    "                                     'context_sent_ja2', 'context_sent_en2', 'context_sent_ja3', 'context_sent_en3','audio'])             #create the vobab dataframe with column names\n",
    "radicaldf = pd.DataFrame(columns = ['id','object','updated','url','level','characters','meanings','lesson_pos', 'char_img',\n",
    "                                   'mean_mne'])                                                                #create the radical dataframe with column names\n",
    "#kmax = -100\n",
    "#kmin = 100\n",
    "#vmax = -100\n",
    "#vmin = 100\n",
    "#rmax = -100\n",
    "#rmin = 100\n",
    "for lv in range(levelst,levele+1):      #go through the levels\n",
    "    print(lv)                           #track the progress of the loop\n",
    "    \n",
    "    data = get_data(lv)                 #call the function to get the data for the desired level\n",
    "    kan, rad, voc = det_type(data)      #determine the type of each element(kanji, radical, vocabulary)\n",
    "    \n",
    "    #tempkan, kanmaxi, kanmini = create_kanjidf(kan, data)       #temporary dataframe to help in analyzing the data to be used for the anki port\n",
    "    #if kmax < kanmaxi:\n",
    "    #    kmax = kanmaxi\n",
    "    #if kmin > kanmini:\n",
    "    #    kmin = kanmini\n",
    "    #tempvoc, vocmaxi, vocmini = create_vocabdf(voc, data)\n",
    "    #if vmax < vocmaxi:\n",
    "    #    vmax = vocmaxi\n",
    "    #if vmin > vocmini:\n",
    "    #    vmin = vocmini\n",
    "    #temprad, radmaxi, radmini = create_radicaldf(rad, data)\n",
    "    #if rmax < radmaxi:\n",
    "    #    rmax = radmaxi\n",
    "    #if rmin > radmini:\n",
    "    #    rmin = radmini\n",
    "    kanjidf = kanjidf.append(create_kanjidf(kan,data))         #call the function to get the data for the kanji dataframe for the current level and append it to the previous levels\n",
    "    vocabdf = vocabdf.append(create_vocabdf(voc, data))        #call the function to get the data for the vocab dataframe for the current level and append it to the previous levels\n",
    "    radicaldf = radicaldf.append(create_radicaldf(rad, data))   #call the function to get the data for the radical dataframe for the current level and append it to the previous levels\n",
    "    \n",
    "#print(#\"\\nkanji\\nmax = \" + str(kmax) + \" min = \" + str(kmin) +              max = 5 min = 1\n",
    "#      \"\\nvocab\\nmax = \" + str(vmax) + \" min = \" + str(vmin))# +              max = 9 min = 1\n",
    "#      \"\\nradical\\nmax = \" + str(rmax) + \" min = \" + str(rmin))             max = 1 min = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "kanjidf.to_csv('kanjidf.csv')\n",
    "vocabdf.to_csv('vocabdf.csv')\n",
    "radicaldf.to_csv('radicaldf.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(radicaldf)\n",
    "#print(vocabdf['aux_mean']) #fix separate urls in audio\n",
    "#print(kanjidf) #meanings, aux_mean, readings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code to pull images from the wanikani website\n",
    "#file = requests.get('https://cdn.wanikani.com/audios/27959-subject-2467.ogg?1553788696', headers = {\"Authorization\": \"Bearer c61d70a7-c134-4b4a-bb05-f97a7c59af8b\"})\n",
    "#with open('/Users/peace/Downloads/audio.mp3', 'wb') as f:\n",
    "#    f.write(file.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dt = get_data(1)\n",
    "#k,r,v = det_type(dt)\n",
    "#raddf = pd.DataFrame(columns = ['id','object','updated','url','level','characters','meanings','lesson_pos', 'char_img',\n",
    "#                                   'mean_mne'])                                                                #create the radical dataframe with column names\n",
    "#for item in r:\n",
    "#    print(1)\n",
    "#    print(dt[item]['data']['character_images'])\n",
    "#    print(dt[item]['data']['characters'])\n",
    "#    if dt[item]['data']['meanings']['primary'] == True:\n",
    "#    print(dt[item]['data']['meanings'])\n",
    "#    print(dt[item]['data'])\n",
    "#raddf = raddf.append(create_radicaldf(r, dt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for lv in range(levelst,levele+1):\n",
    "#    response = requests.get(\"https://api.wanikani.com/v2/subjects?levels=\"+ str(lv), headers = {\"Authorization\": \"Bearer c61d70a7-c134-4b4a-bb05-f97a7c59af8b\"})\n",
    "#    dj = response.json()    #retrieve the json from the website\n",
    "#    dja = dj['data']     #use only the data returned \n",
    "#    print(dja[0]['data']['level'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
